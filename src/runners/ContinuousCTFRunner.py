import torch
import logging
from time import time
from utils import utils
from utils.global_p import *
from tqdm import tqdm
import gc
import numpy as np
import copy
import os
from runners.BaseRunner import BaseRunner
import math
from scipy.special import gamma

import pdb

class ContinuousCTFRunner(BaseRunner):
    def parse_ctf_runner_args(parser):
        
        parser.add_argument('--ctf_load', type=int, default=0,
                            help='Whether load model with ctf_load and continue to train')
        parser.add_argument('--ctf_train', type=int, default=1,
                            help='Whether train model with ctf_load')
        parser.add_argument('--ctf_num', type=int, default=100,
                            help='The number of counterfactual history for each sample')
        parser.add_argument('--epsilon1', type=float, default=0.0,
                            help='The accepted error for counterfactual constraints')
        parser.add_argument('--epsilon2', type=float, default=0.0,
                            help='The accepted error for counterfactual constraints')
        parser.add_argument('--cc_weight', type=float, default=0.1,
                            help='The weight of counterfactual constraint')
        parser.add_argument('--check_ctf_loss', type=int, default=0,
                            help='whether check ctf_loss')
        return BaseRunner.parse_runner_args(parser)
    
    def __init__(self, epsilon1, epsilon2, cc_weight, ctf_num, check_ctf_loss, *args, **kwargs):
        
        self.epsilon1 = epsilon1 # control how rigorous is the constraint
        self.cc_weight = cc_weight # weight of counterfactual constraint loss
        self.epsilon2 = epsilon2 # control continuous counterfactual examples distance
        self.ctf_num = ctf_num # number of counterfactual examples
        self.check_ctf_loss = check_ctf_loss # whether check ctf_loss
        BaseRunner.__init__(self, *args, **kwargs)
    
    def fit(self, model, data, data_processor, epoch=-1):  # fit the results for an input set
        """
        training
        :param model: model
        :param data: data dictï¼Œgenerated by self.get_*_data() and self.format_data_dict() from DataProcessor
        :param data_processor: DataProcessor instant
        :param epoch: epoch number
        :return: return output of last epoch, could provide self.check to check intermediate results.
        """
        gc.collect()
        if model.optimizer is None:
            model.optimizer = self._build_optimizer(model)
        batches = data_processor.prepare_batches(data, self.batch_size, train=True, model=model)
        batches = self.batches_add_control(batches, train=True)
        if self.pre_gpu == 1:
            batches = [data_processor.batch_to_gpu(b) for b in batches]

        batch_size = self.batch_size if data_processor.rank == 0 else self.batch_size * 2
        model.train()
        accumulate_size, prediction_list, output_dict = 0, [], None
        loss_list, loss_l2_list = [], []
        for i, batch in \
                tqdm(list(enumerate(batches)), leave=False, desc='Epoch %5d' % (epoch + 1), ncols=100, mininterval=1):
            if self.pre_gpu == 0:
                batch = data_processor.batch_to_gpu(batch)
            accumulate_size += len(batch[Y])
            model.optimizer.zero_grad()
            output_dict = model(batch, self.ctf_num, self.epsilon2)
            l2 = output_dict[LOSS_L2]
            loss = output_dict[LOSS] + l2 * self.l2s_weight
            ctf_loss = self.integrate(output_dict[PREDICTION], output_dict[CTF_PREDICTION], output_dict[CTF_HIS_DIST], output_dict[DIM])
            ccf_loss = torch.norm(torch.max(torch.zeros_like(output_dict[PREDICTION]), ctf_loss - self.epsilon1))
            loss += self.cc_weight * ccf_loss
            if self.check_ctf_loss:
                pdb.set_trace()
            loss.backward()
            loss_list.append(loss.detach().cpu().data.numpy())
            loss_l2_list.append(l2.detach().cpu().data.numpy())
            prediction_list.append(output_dict[PREDICTION].detach().cpu().data.numpy()[:batch[REAL_BATCH_SIZE]])
            if self.grad_clip > 0:
                # torch.nn.utils.clip_grad_norm_(model.parameters(), 100)
                torch.nn.utils.clip_grad_value_(model.parameters(), self.grad_clip)
            if accumulate_size >= batch_size or i == len(batches) - 1:
                model.optimizer.step()
                accumulate_size = 0
            # model.optimizer.step()
        model.eval()
        gc.collect()

        predictions = np.concatenate(prediction_list)
        sample_ids = np.concatenate([b[SAMPLE_ID][:b[REAL_BATCH_SIZE]] for b in batches])
        reorder_dict = dict(zip(sample_ids, predictions))
        predictions = np.array([reorder_dict[i] for i in data[SAMPLE_ID]])
        return predictions, output_dict, np.mean(loss_list), np.mean(loss_l2_list)
    
    def integrate(self, predictions, ctf_predictions, ctf_his_dist, dim):
        con = math.pi ** (dim/2) / gamma(dim/2+1)
        ctf_his_dist = ctf_his_dist.transpose(1,0)
        ctf_predictions = ctf_predictions.transpose(1,0)                                        
        sorted_dist, indices = torch.sort(ctf_his_dist, dim=0)
        rearrange_ctf_prediction = ctf_predictions.scatter(dim=0, index=indices.to(ctf_predictions.device), src=ctf_predictions)
        dist = torch.zeros_like(sorted_dist)
        dist[1:,:]=sorted_dist[:-1,:]
        gaps = sorted_dist - dist
        ring = con * (sorted_dist ** dim - dist ** dim).to(predictions.device)
        integrate = ring * (ctf_predictions - predictions).abs()
        integrate = integrate.sum(dim=0)
        return integrate
        
    